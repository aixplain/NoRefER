{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from norefer import *\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of filenames\n",
    "filenames = ['en-libre.csv', 'en-common.csv', 'es-common.csv', 'fr-common.csv']\n",
    "folder_path = '../dataset/'\n",
    "\n",
    "# List to store data from each file\n",
    "all_data = []\n",
    "\n",
    "# Loop through each file and load data\n",
    "for filename in filenames:\n",
    "    print(f'Start processing file: {filename}')\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(f\"Data loaded successfully for {filename}.\")\n",
    "        all_data.append(data)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading {filename}:\", e)\n",
    "        continue\n",
    "\n",
    "# Concatenate all data into a single DataFrame\n",
    "combined_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Process the combined data\n",
    "transcription = combined_data['outputText'].astype(str).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the data to extract the attentions and error labels on word level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_attention = process_transcription_attention(transcription, need_split=False) \n",
    "data_attention = calculate_word_scores_with_tokens(data_attention, 'adjusted_attentions', aggregation_method='max')  # aggregation_method=['average', 'max', 'q3']\n",
    "\n",
    "data_attention ['inputPath'] = combined_data['inputText']\n",
    "data_attention ['referenceText'] = combined_data['referenceText']\n",
    "\n",
    "b_score_word = get_word_fault_scores_jiwer(list(data_attention['referenceText']), list(data_attention['outputText']))\n",
    "data_attention['jiwer_scores'] = b_score_word\n",
    "\n",
    "data_attention['actualwords'] = data_attention['jiwer_scores'].apply(lambda x: [item[0] for item in x])\n",
    "data_attention['word_jiwer_score'] = data_attention['jiwer_scores'].apply(lambda x: [item[1] for item in x])\n",
    "data_attention['word_attentions_aligned'] = align_attention_with_jiwer(data_attention['word_jiwer_score'], data_attention['word_attentions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the gradients on token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_attention['TokenGradiants'] = process_transcription_gradient(transcription)\n",
    "data_attention.to_csv('../dataset/alldata_attention_gradient_withIndex.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the word level gradient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_attention = calculate_word_scores_with_tokens_grad(data_attention, 'TokenGradiants', aggregation_method='min') \n",
    "data_attention['word_grad_aligned'] = align_attention_with_jiwer(data_attention['word_jiwer_score'], data_attention['word_grad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given scaled attention values and the gradient for each word, we calculate the Attentin X Gardient to compare againt the proposed method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_vectors(row):\n",
    "    grad = row['word_grad_aligned']\n",
    "    atten = row['word_attentions_aligned']\n",
    "\n",
    "    # Ensure grad and atten are lists of numbers\n",
    "    if isinstance(grad, str):\n",
    "        grad = ast.literal_eval(grad)\n",
    "    if isinstance(atten, str):\n",
    "        atten = ast.literal_eval(atten)\n",
    "\n",
    "    # Convert elements to floats, treating 'None' as 0.0\n",
    "    grad = [float(g) if isinstance(g, str) and g != 'None' else 0.0 for g in grad]\n",
    "    atten = [float(a) if isinstance(a, str) and a != 'None' else 0.0 for a in atten]\n",
    "\n",
    "    return [-1 * g * a for g, a in zip(grad, atten)]\n",
    "\n",
    "data_attention['gradXatten'] = data_attention.apply(multiply_vectors, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence level analysis\n",
    "Assessing the effectiveness of attention values by comparing them with actual discrepancies identified through comparison with reference sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, classification_report, balanced_accuracy_score, average_precision_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_scores_and_attentions(word_jiwer_scores, word_attentions):\n",
    "    valid_scores = []\n",
    "    valid_attentions = []\n",
    "    for jiwer_score, attention in zip(word_jiwer_scores, word_attentions):\n",
    "        if jiwer_score not in [2]:  # Excluding deletion (2)\n",
    "            if attention is not None:\n",
    "                valid_scores.append(1 if jiwer_score != 0 else 0)  # Convert to binary label\n",
    "                valid_attentions.append(attention)\n",
    "    return valid_scores, valid_attentions\n",
    "\n",
    "auc_scores = []\n",
    "\n",
    "for index, row in data_attention.iterrows():\n",
    "    valid_scores, valid_attentions = get_valid_scores_and_attentions(row['word_jiwer_score'], row['gradXatten'])\n",
    "\n",
    "    if len(valid_scores) > 1 and len(valid_attentions) > 1:\n",
    "        try:\n",
    "            auc_score = roc_auc_score(valid_scores, valid_attentions)\n",
    "            auc_scores.append(auc_score)\n",
    "        except ValueError as e:\n",
    "            # Handle case where only one class is present in y_true\n",
    "            # print(f\"Row {index} skipped: {e}\")\n",
    "            pass\n",
    "\n",
    "average_auc_score = np.nanmean(auc_scores)\n",
    "print(\"Average AUC Score: \", average_auc_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_scores_and_attentions(word_jiwer_scores, word_attentions):\n",
    "    valid_scores = []\n",
    "    valid_attentions = []\n",
    "    for jiwer_score, attention in zip(word_jiwer_scores, word_attentions):\n",
    "        if jiwer_score not in [2]:  # Excluding deletion (2) \n",
    "            if attention is not None:\n",
    "                # Ensure that jiwer_score is a valid integer and attention is a valid float\n",
    "                try:\n",
    "                    valid_scores.append(1 if int(jiwer_score) != 0 else 0)  # Convert to binary label\n",
    "                    valid_attentions.append(float(attention))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    return valid_scores, valid_attentions\n",
    "\n",
    "average_precision_scores = []\n",
    "\n",
    "for index, row in data_attention.iterrows():\n",
    "    valid_scores, valid_attentions = get_valid_scores_and_attentions(row['word_jiwer_score'], row['gradXatten'])\n",
    "\n",
    "    if len(valid_scores) > 1 and len(valid_attentions) > 1:\n",
    "        try:\n",
    "            ap_score = average_precision_score(valid_scores, valid_attentions, average='weighted')\n",
    "            average_precision_scores.append(ap_score)\n",
    "        except ValueError as e:\n",
    "            # Handle cases where only one class is present or other issues\n",
    "            # print(f\"Row {index} skipped: {e}\")\n",
    "            pass\n",
    "\n",
    "average_ap_score = np.nanmean(average_precision_scores)\n",
    "print(\"Average AP Score: \", average_ap_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate top k classification metrics - dynamic k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_top_k_attention_words(word_jiwer_scores, word_attentions, sentence_length):\n",
    "    # Dynamic k based on 10% of sentence length\n",
    "    k = max(1, int(np.ceil(0.10 * sentence_length)))  # Ensure at least 1\n",
    "    numeric_attentions = [float(att) if att not in [None, 'None'] and isinstance(att, (float, str, int)) else 0 for att in word_attentions]\n",
    "    paired_scores = sorted(zip(word_jiwer_scores, numeric_attentions), key=lambda x: x[1])\n",
    "    \n",
    "    binary_labels = [1 if score[0] != 0 else 0 for score in paired_scores]  # Convert word_jiwer_scores to binary\n",
    "    binary_predictions = [1 if i < k else 0 for i in range(len(paired_scores))]  # Top k words are faulty\n",
    "\n",
    "    return binary_labels, binary_predictions\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "baccuracy_scores = []\n",
    "\n",
    "# Process each row\n",
    "for index, row in data_attention.iterrows():\n",
    "    word_jiwer_scores = row['word_jiwer_score']\n",
    "    word_attentions = row['gradXatten']\n",
    "    sentence_length = len(word_attentions)  \n",
    "\n",
    "    binary_labels, binary_predictions = classify_top_k_attention_words(word_jiwer_scores, word_attentions, sentence_length)\n",
    "\n",
    "    report = classification_report(binary_labels, binary_predictions, output_dict=True, zero_division=0)\n",
    "    acc = balanced_accuracy_score(binary_labels, binary_predictions)\n",
    "\n",
    "    precision_scores.append(report['weighted avg']['precision'])\n",
    "    recall_scores.append(report['weighted avg']['recall'])\n",
    "    f1_scores.append(report['weighted avg']['f1-score'])\n",
    "    accuracy_scores.append(report['accuracy'])\n",
    "    baccuracy_scores.append(acc)\n",
    "\n",
    "# Calculate mean of metrics\n",
    "mean_precision = np.mean(precision_scores)\n",
    "mean_recall = np.mean(recall_scores)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "mean_baccuracy = np.mean(baccuracy_scores)\n",
    "\n",
    "print(f\"Mean Precision: {mean_precision}\")\n",
    "print(f\"Mean Recall: {mean_recall}\")\n",
    "print(f\"Mean F1 Score: {mean_f1}\")\n",
    "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
    "print(f\"Mean Balanced Accuracy: {mean_baccuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asrvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
