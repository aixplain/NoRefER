{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from norefer import *\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../dataset/en-libre.csv' \n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"Data loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n",
    "data.head() if 'data' in locals() else \"Data not loaded.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract attentions and find the best way to normalize them:\n",
    "\n",
    "- here we used the scaled attentions with norm of value vecotrs, you can check the function to investigate it.\n",
    "- norm_method argument, will add additional normalization method on top of norm of value vectors. possible values for this argument is given in comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = data['outputText'].astype(str).to_list()\n",
    "data_attention = process_transcription_attention(transcription, norm_method='none', need_split=False)  # norm_method=['normalize','standardize', 'none']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word attentions are calculated from coresponding tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_attention = calculate_word_scores_with_tokens(data_attention, 'norm_avg_attentions', aggregation_method='max')  # aggregation_method=['average', 'max', 'q3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a categorical label for each word to show following categories:\n",
    "\n",
    "- 0 --- Correct words\n",
    "- 1 --- Subsittude words\n",
    "- 2 --- deleted words\n",
    "- 3 --- inserted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_attention ['inputPath'] = data['inputText']\n",
    "data_attention ['referenceText'] = data['referenceText']\n",
    "\n",
    "b_score_word = get_word_fault_scores_jiwer(list(data_attention['referenceText']), list(data_attention['outputText']))\n",
    "data_attention['jiwer_scores'] = b_score_word\n",
    "\n",
    "data_attention['actualwords'] = data_attention['jiwer_scores'].apply(lambda x: [item[0] for item in x])\n",
    "data_attention['word_jiwer_score'] = data_attention['jiwer_scores'].apply(lambda x: [item[1] for item in x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we align the attention values and these scores and save them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_attention['word_attentions_aligned'] = align_attention_with_jiwer(data_attention['word_jiwer_score'], data_attention['word_attentions'])\n",
    "data_attention.to_csv('./data/en-libre_attention_withIndex.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking words based on their attention values (sentence level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, classification_report, balanced_accuracy_score, average_precision_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_scores_and_attentions(word_jiwer_scores, word_attentions):\n",
    "    valid_scores = []\n",
    "    valid_attentions = []\n",
    "    for jiwer_score, attention in zip(word_jiwer_scores, word_attentions):\n",
    "        if jiwer_score not in [2]:  # Excluding deletion (2)\n",
    "            if attention is not None:\n",
    "                valid_scores.append(1 if jiwer_score != 0 else 0)  # Convert to binary label\n",
    "                valid_attentions.append(attention)\n",
    "    return valid_scores, valid_attentions\n",
    "\n",
    "auc_scores = []\n",
    "\n",
    "for index, row in data_attention.iterrows():\n",
    "    valid_scores, valid_attentions = get_valid_scores_and_attentions(row['word_jiwer_score'], row['word_attentions_aligned'])\n",
    "\n",
    "    if len(valid_scores) > 1 and len(valid_attentions) > 1:\n",
    "        try:\n",
    "            auc_score = roc_auc_score(valid_scores, valid_attentions)\n",
    "            auc_scores.append(auc_score)\n",
    "        except ValueError as e:\n",
    "            # Handle case where only one class is present in y_true\n",
    "            # print(f\"Row {index} skipped: {e}\")\n",
    "            pass\n",
    "\n",
    "average_auc_score = np.nanmean(auc_scores)\n",
    "print(\"Average AUC Score: \", average_auc_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_scores_and_attentions(word_jiwer_scores, word_attentions):\n",
    "    valid_scores = []\n",
    "    valid_attentions = []\n",
    "    for jiwer_score, attention in zip(word_jiwer_scores, word_attentions):\n",
    "        if jiwer_score not in [2]:  # Excluding deletion (2) and insertion (3)\n",
    "            if attention is not None:\n",
    "                # Ensure that jiwer_score is a valid integer and attention is a valid float\n",
    "                try:\n",
    "                    valid_scores.append(1 if int(jiwer_score) != 0 else 0)  # Convert to binary label\n",
    "                    valid_attentions.append(float(attention))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    return valid_scores, valid_attentions\n",
    "\n",
    "average_precision_scores = []\n",
    "\n",
    "for index, row in data_attention.iterrows():\n",
    "    valid_scores, valid_attentions = get_valid_scores_and_attentions(row['word_jiwer_score'], row['word_attentions_aligned'])\n",
    "\n",
    "    if len(valid_scores) > 1 and len(valid_attentions) > 1:\n",
    "        try:\n",
    "            ap_score = average_precision_score(valid_scores, valid_attentions)\n",
    "            average_precision_scores.append(ap_score)\n",
    "        except ValueError as e:\n",
    "            # Handle cases where only one class is present or other issues\n",
    "            # print(f\"Row {index} skipped: {e}\")\n",
    "            pass\n",
    "\n",
    "average_ap_score = np.nanmean(average_precision_scores)\n",
    "print(\"Average AP Score: \", average_ap_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate top k classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_top_k_attention_words(word_jiwer_scores, word_attentions, k):\n",
    "    numeric_attentions = [float(att) if att not in [None, 'None'] and isinstance(att, (float, str, int)) else 0 for att in word_attentions]\n",
    "    paired_scores = sorted(zip(word_jiwer_scores, numeric_attentions), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    binary_labels = [1 if score[0] != 0 else 0 for score in paired_scores] # Convert word_jiwer_scores to binary\n",
    "    binary_predictions = [1 if i < k else 0 for i in range(len(paired_scores))]  # Top k words are faulty\n",
    "\n",
    "    return binary_labels, binary_predictions\n",
    "\n",
    "# Define k value\n",
    "k = 2 # Adjust k value as needed\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "baccuracy_scores = []\n",
    "\n",
    "# Process each row\n",
    "for index, row in data_attention.iterrows():\n",
    "    word_jiwer_scores = row['word_jiwer_score']\n",
    "    word_attentions = row['word_attentions_aligned']\n",
    "\n",
    "    binary_labels, binary_predictions = classify_top_k_attention_words(word_jiwer_scores, word_attentions, k)\n",
    "\n",
    "\n",
    "    report = classification_report(binary_labels, binary_predictions, output_dict=True, zero_division=0)\n",
    "    acc = balanced_accuracy_score(binary_labels, binary_predictions)\n",
    "\n",
    "    precision_scores.append(report['weighted avg']['precision'])\n",
    "    recall_scores.append(report['weighted avg']['recall'])\n",
    "    f1_scores.append(report['weighted avg']['f1-score'])\n",
    "    accuracy_scores.append(report['accuracy'])\n",
    "    baccuracy_scores.append(acc)\n",
    "\n",
    "# Calculate mean of metrics\n",
    "mean_precision = np.mean(precision_scores)\n",
    "mean_recall = np.mean(recall_scores)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "mean_baccuracy = np.mean(baccuracy_scores)\n",
    "\n",
    "print(f\"Mean Precision: {mean_precision}\")\n",
    "print(f\"Mean Recall: {mean_recall}\")\n",
    "print(f\"Mean F1 Score: {mean_f1}\")\n",
    "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
    "print(f\"Mean Balanced Accuracy: {mean_baccuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate top k classification metrics - dynamic k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_top_k_attention_words(word_jiwer_scores, word_attentions, sentence_length):\n",
    "    # Dynamic k based on 10% of sentence length\n",
    "    k = max(1, int(np.ceil(0.10 * sentence_length)))  # Ensure at least 1\n",
    "    numeric_attentions = [float(att) if att not in [None, 'None'] and isinstance(att, (float, str, int)) else 0 for att in word_attentions]\n",
    "    paired_scores = sorted(zip(word_jiwer_scores, numeric_attentions), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    binary_labels = [1 if score[0] != 0 else 0 for score in paired_scores]  # Convert word_jiwer_scores to binary\n",
    "    binary_predictions = [1 if i < k else 0 for i in range(len(paired_scores))]  # Top k words are faulty\n",
    "\n",
    "    return binary_labels, binary_predictions\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "baccuracy_scores = []\n",
    "\n",
    "# Process each row\n",
    "for index, row in data_attention.iterrows():\n",
    "    word_jiwer_scores = row['word_jiwer_score']\n",
    "    word_attentions = row['word_attentions_aligned']\n",
    "    sentence_length = len(word_attentions)  \n",
    "\n",
    "    binary_labels, binary_predictions = classify_top_k_attention_words(word_jiwer_scores, word_attentions, sentence_length)\n",
    "\n",
    "    report = classification_report(binary_labels, binary_predictions, output_dict=True, zero_division=0)\n",
    "    acc = balanced_accuracy_score(binary_labels, binary_predictions)\n",
    "\n",
    "    precision_scores.append(report['weighted avg']['precision'])\n",
    "    recall_scores.append(report['weighted avg']['recall'])\n",
    "    f1_scores.append(report['weighted avg']['f1-score'])\n",
    "    accuracy_scores.append(report['accuracy'])\n",
    "    baccuracy_scores.append(acc)\n",
    "\n",
    "# Calculate mean of metrics\n",
    "mean_precision = np.mean(precision_scores)\n",
    "mean_recall = np.mean(recall_scores)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "mean_baccuracy = np.mean(baccuracy_scores)\n",
    "\n",
    "print(f\"Mean Precision: {mean_precision}\")\n",
    "print(f\"Mean Recall: {mean_recall}\")\n",
    "print(f\"Mean F1 Score: {mean_f1}\")\n",
    "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
    "print(f\"Mean Balanced Accuracy: {mean_baccuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking words based on their attention values (dataset level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/en-libre_attention_withIndex.csv'\n",
    "data_attention = pd.read_csv(file_path)\n",
    "data_attention.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_df = expand_and_rank_words(data_attention, 'words', 'word_attentions_aligned', 'word_jiwer_score')\n",
    "ranked_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = ranked_df['jiwer'].value_counts()\n",
    "value_counts = ranked_df['jiwer'].value_counts().reindex([0, 1, 2, 3], fill_value=0)\n",
    "\n",
    "print('Number of correct, substituted, deleted, and inserted words in the dataset')\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check the most frequent faulty words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming ranked_df is your existing dataframe with the mentioned columns\n",
    "\n",
    "Example structure of ranked_df: ['Word', 'Attention', 'jiwer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Word' and aggregate the attention values and jiwer scores into lists\n",
    "aggregated_df = ranked_df.groupby('Word').agg({\n",
    "    'Attention': lambda x: list(x),\n",
    "    'jiwer': lambda x: list(x)\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate the frequency of faulty scores for each word\n",
    "aggregated_df['Faulty_Frequency'] = aggregated_df['jiwer'].apply(lambda x: sum(y > 0 for y in x))\n",
    "\n",
    "# Sort the dataframe based on the frequency of faulty scores (descending order)\n",
    "aggregated_df = aggregated_df.sort_values(by='Faulty_Frequency', ascending=False)\n",
    "\n",
    "# Reset the index of the dataframe\n",
    "aggregated_df = aggregated_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the 'jiwer' column by changing 1, 2, 3 to 1 and keeping 0 as is\n",
    "aggregated_df['jiwer'] = aggregated_df['jiwer'].apply(lambda scores: [1 if score > 0 else 0 for score in scores])\n",
    "\n",
    "# Add a new column for the ratio of actual errors\n",
    "aggregated_df['ratio_of_actual_errors'] = aggregated_df.apply(lambda row: row['Faulty_Frequency'] / len(row['jiwer']) if len(row['jiwer']) > 0 else 0, axis=1)\n",
    "\n",
    "# Add a new column for the average of attention values\n",
    "aggregated_df['average_attention'] = aggregated_df['Attention'].apply(lambda attentions: sum(attentions) / len(attentions) if attentions else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the average attention where jiwer value is not 0\n",
    "def calculate_filtered_attention_average(attentions, jiwer_scores):\n",
    "    filtered_attentions = [attention for attention, score in zip(attentions, jiwer_scores) if score != 0]\n",
    "    return sum(filtered_attentions) / len(filtered_attentions) if filtered_attentions else 0\n",
    "\n",
    "# Apply the function to each row of the DataFrame\n",
    "aggregated_df['filtered_average_attention'] = aggregated_df.apply(\n",
    "    lambda row: calculate_filtered_attention_average(row['Attention'], row['jiwer']),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the correlation between filtered_average_attention and ratio_of_actual_errors\n",
    "correlation_rae = aggregated_df['filtered_average_attention'].corr(aggregated_df['ratio_of_actual_errors'], method='pearson')\n",
    "print(\"Correlation between Filtered Average Attention and Ratio of Actual Errors:\", correlation_rae)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asrvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
