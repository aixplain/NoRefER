{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a4c0cbb",
   "metadata": {},
   "source": [
    "The material in this notebook is partialy adopted from the following toturial:\n",
    "\n",
    "https://github.com/NVIDIA/NeMo/blob/main/tutorials/asr/ASR_Confidence_Estimation.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fad9424",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "import liberaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76fbff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from typing import Any, List, Text, Tuple, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb47852",
   "metadata": {},
   "source": [
    "Assuming to have the audio data dowloded in following directory:\n",
    "\n",
    "/NoRefER/audio_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc592ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = '/NoRefER/'\n",
    "DATA_DIR = WORK_DIR + '/audio_data/'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d34787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of filenames\n",
    "filenames = ['en-libre.csv', 'en-common.csv', 'es-common.csv', 'fr-common.csv']\n",
    "folder_path = '../data/'\n",
    "\n",
    "# List to store data from each file\n",
    "all_data = []\n",
    "\n",
    "# Loop through each file and load data\n",
    "for filename in filenames:\n",
    "    print(f'Start processing file: {filename}')\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(f\"Data loaded successfully for {filename}.\")\n",
    "        all_data.append(data)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading {filename}:\", e)\n",
    "        continue\n",
    "\n",
    "# Concatenate all data into a single DataFrame\n",
    "combined_data = pd.concat(all_data, ignore_index=True)\n",
    "audio_path_s3 = combined_data['inputText'].astype(str).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe9913d",
   "metadata": {
    "id": "1a0f93c6"
   },
   "outputs": [],
   "source": [
    "BRANCH = 'main'\n",
    "\n",
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd26974d",
   "metadata": {
    "id": "ffdfe626"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# either provide a path to local NeMo repository with NeMo already installed or git clone\n",
    "\n",
    "# option #1: local path to NeMo repo with NeMo already installed\n",
    "NEMO_DIR_PATH = os.path.dirname(os.path.dirname(os.path.abspath('')))\n",
    "is_colab = False\n",
    "\n",
    "# option #2: download NeMo repo\n",
    "if 'google.colab' in str(get_ipython()) or not os.path.exists(os.path.join(NEMO_DIR_PATH, \"nemo\")):\n",
    "    ## Install dependencies\n",
    "    !apt-get install sox libsndfile1 ffmpeg\n",
    "\n",
    "    !git clone -b $BRANCH https://github.com/NVIDIA/NeMo\n",
    "    %cd NeMo\n",
    "    !python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n",
    "    NEMO_DIR_PATH = os.path.abspath('')\n",
    "    is_colab = True\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, NEMO_DIR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed997bfd",
   "metadata": {
    "id": "dec57a27"
   },
   "source": [
    "## Data and model loading\n",
    "This tutorial uses CTC Conformer models trained on LibriSpeech.\n",
    "\n",
    "You can try to use other pre-trained models as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1a27a",
   "metadata": {
    "id": "b66c60a3"
   },
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig, OmegaConf\n",
    "from nemo.collections.asr.models import ASRModel\n",
    "\n",
    "def load_model(name: str):\n",
    "    \"\"\"Load a pre-trained model.\n",
    "\n",
    "    Args:\n",
    "        name: Pre-trained model name.\n",
    "            Reserved names:\n",
    "            - 'ctc' for 'stt_en_conformer_ctc_large_ls'\n",
    "\n",
    "    Returns:\n",
    "        A model loaded into GPU with .eval() mode set.\n",
    "    \"\"\"\n",
    "    if name == \"ctc\":\n",
    "        name = \"stt_en_conformer_ctc_large_ls\"\n",
    "    elif name == \"rnnt\":\n",
    "        name = \"stt_en_conformer_transducer_large_ls\"\n",
    "\n",
    "    model = ASRModel.from_pretrained(model_name=name, map_location=\"cuda:0\")\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Load model\n",
    "is_rnnt=False\n",
    "model = load_model(\"ctc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c56c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TestSet:\n",
    "    filepaths: List[str]\n",
    "    reference_texts: List[str]\n",
    "\n",
    "def load_data(df):\n",
    "    filepaths = []\n",
    "    reference_texts = []\n",
    "    durations = []\n",
    "    for index, row in df.iterrows():\n",
    "        audio_file = row['local_paths']\n",
    "        filepaths.append(str(audio_file))\n",
    "        text = row['referenceText']\n",
    "        reference_texts.append(text)\n",
    "    return TestSet(filepaths, reference_texts)\n",
    "\n",
    "# Load data\n",
    "test_sets =  load_data(combined_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5db700",
   "metadata": {
    "id": "88c3d7ee"
   },
   "source": [
    "## Setting up confidence estimation\n",
    "To set up confidence estimation for NeMo ASR models, you need to:\n",
    "1. Initialize _ConfidenceConfig_\n",
    "2. Put the created _ConfidenceConfig_ into the model decoding config.\n",
    "\n",
    "The following cell contains an example of _ConfidenceConfig_ initialization and updating the model's decoding config.\n",
    "\n",
    "For the _ConfidenceConfig_ there are also listed possible values for its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3e8c11",
   "metadata": {
    "id": "078005f1"
   },
   "outputs": [],
   "source": [
    "from nemo.collections.asr.parts.submodules.rnnt_decoding import RNNTDecodingConfig\n",
    "from nemo.collections.asr.parts.submodules.ctc_decoding import CTCDecodingConfig\n",
    "from nemo.collections.asr.parts.utils.asr_confidence_utils import (\n",
    "    ConfidenceConfig,\n",
    "    ConfidenceConstants,\n",
    "    ConfidenceMethodConfig,\n",
    "    ConfidenceMethodConstants,\n",
    ")\n",
    "from nemo.collections.asr.parts.utils.asr_confidence_benchmarking_utils import (\n",
    "    apply_confidence_parameters,\n",
    "    get_correct_marks,\n",
    "    get_token_targets_with_confidence,\n",
    "    get_word_targets_with_confidence,\n",
    ")\n",
    "\n",
    "\n",
    "# List allowed options for ConfidenceMethodConfig and ConfidenceConfig\n",
    "print(f\"Allowed options for ConfidenceMethodConfig: {ConfidenceMethodConstants.print()}\\n\")\n",
    "print(f\"Allowed options for ConfidenceConfig: {ConfidenceConstants.print()}\\n\")\n",
    "\n",
    "# Initialize ConfidenceConfig and ConfidenceMethodConfig\n",
    "confidence_cfg = ConfidenceConfig(\n",
    "    preserve_frame_confidence=True, # Internally set to true if preserve_token_confidence == True\n",
    "    # or preserve_word_confidence == True\n",
    "    preserve_token_confidence=True, # Internally set to true if preserve_word_confidence == True\n",
    "    preserve_word_confidence=True,\n",
    "    aggregation=\"prod\", # How to aggregate frame scores to token scores and token scores to word scores\n",
    "    exclude_blank=False, # If true, only non-blank emissions contribute to confidence scores\n",
    "    method_cfg=ConfidenceMethodConfig( # Config for per-frame scores calculation (before aggregation)\n",
    "        name=\"max_prob\", # Or \"entropy\" (default), which usually works better\n",
    "        entropy_type=\"gibbs\", # Used only for name == \"entropy\". Recommended: \"tsallis\" (default) or \"renyi\"\n",
    "        alpha=0.5, # Low values (<1) increase sensitivity, high values decrease sensitivity\n",
    "        entropy_norm=\"lin\" # How to normalize (map to [0,1]) entropy. Default: \"exp\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Alternalively, look at ConfidenceConfig's docstring\n",
    "print(f\"More info on ConfidenceConfig here:\\n{ConfidenceConfig().__doc__}\\n\")\n",
    "\n",
    "# Put the created ConfidenceConfig into the model decoding config via .change_decoding_strategy()\n",
    "model.change_decoding_strategy(\n",
    "    RNNTDecodingConfig(fused_batch_size=-1, strategy=\"greedy_batch\", confidence_cfg=confidence_cfg)\n",
    "    if is_rnnt\n",
    "    else CTCDecodingConfig(confidence_cfg=confidence_cfg)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04581687",
   "metadata": {
    "id": "efe0baea"
   },
   "source": [
    "## Decode test set and get transcriptions with confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f92257",
   "metadata": {
    "id": "ccd8d0de"
   },
   "outputs": [],
   "source": [
    "transcriptions = model.transcribe(paths2audio_files=test_sets.filepaths, batch_size=16, return_hypotheses=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ba6baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_word = []\n",
    "data_transcriptions = []\n",
    "for tran in transcriptions:\n",
    "    instance = [round(c, 3) for c in tran.word_confidence]\n",
    "    confidence_word.append(instance)\n",
    "    data_transcriptions.append(tran.text)\n",
    "\n",
    "combined_data['confidence_word'] = confidence_word\n",
    "combined_data['data_transcriptions'] = data_transcriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468ad3e",
   "metadata": {
    "id": "dbfcb2da"
   },
   "source": [
    "## Confidence metrics\n",
    "\n",
    "There are several metrics to evaluate the effectiveness of a confidence estimation method. Some of them consider confidence estimation as a binary classification task. Other measure how close the correct word confidence scores are to $1.0$ and the incorrect word scores are to $0.0$.\n",
    "\n",
    "Some of them are:\n",
    "1. Area Under the Receiver Operating Characteristics Curve ($\\mathrm{AUC}_\\mathrm{ROC}$): class separability metric.\n",
    "2. Area Under the Precision-Recall Curve ($\\mathrm{AUC}_\\mathrm{PR}$): how well the correct words are detected.\n",
    "3. Area Under the Negative Predictive Value vs. True Negative Rate Curve ($\\mathrm{AUC}_\\mathrm{NT}$): how well the incorrect words are detected ($\\mathrm{AUC}_\\mathrm{PR}$ in which errors are treated as positives).\n",
    "4. Normalized Cross Entropy ($\\mathrm{NCE}$): how close of confidence for correct predictions to $1.0$ and of incorrect predictions to $0.0$. It ranges from $-\\infty$ to $1.0$, with negative scores indicating that the conﬁdence method performs worse than the setting confidence score to $1-\\mathrm{WER}$. This metric is also known as Normalized Mutual Information.\n",
    "5. Expected Calibration Error ($\\mathrm{ECE}$): a weighted average over the absolute accuracy/confidence difference. It ranges from $0.0$ to $1.0$ with the best value $0.0$.\n",
    "\n",
    "Metrics based on the Youden's curve (see https://en.wikipedia.org/wiki/Youden%27s_J_statistic) can also be considered. They are:\n",
    "1. Area Under the Youden's curve ($\\mathrm{AUC}_\\mathrm{YC}$): the rate of the effective threshold range (i.e. the adjustability or responsiveness). It ranges from $0.0$ to $1.0$ with the best value $0.5$.\n",
    "2. Maximum of the Youden's curve $\\mathrm{MAX}_\\mathrm{YC}$: the optimal $\\mathrm{TNR}$ vs. $\\mathrm{FNR}$ tradeoff. It's unnormalized version can be used as a criterion for selecting the optimal $\\tau$. It ranges from $0.0$ to $1.0$ with the best value $1.0$.\n",
    "3. The standard deviation of the Youden's curve values ($\\mathrm{STD}_\\mathrm{YC}$): indicates that $\\mathrm{TNR}$ and $\\mathrm{FNR}$ increase at different rates (viz. $\\mathrm{TNR}$ grows faster) as the $\\tau$ increases. It ranges from $0.0$ to $0.5$ with the best value around $0.25$.\n",
    "\n",
    "When selecting/tuning a confidence method, it is recommended to maximize $\\mathrm{AUC}_\\mathrm{ROC}$ first as this is the main metric of confidence estimation quality. Then, for overconfident models, maximizing $\\mathrm{AUC}_\\mathrm{NT}$ should take precedence over $\\mathrm{AUC}_\\mathrm{PR}$. Finally, a trade-off between $\\mathrm{NCE}$/$\\mathrm{ECE}$ and the family of $\\mathrm{YC}$ metrics considered as a compromise between formal correctness and controllability.\n",
    "\n",
    "Let's see how well our confidence performs according to the metrics above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0fa793",
   "metadata": {
    "id": "5d152775"
   },
   "outputs": [],
   "source": [
    "from nemo.collections.asr.parts.utils.confidence_metrics import (\n",
    "    auc_nt,\n",
    "    auc_pr,\n",
    "    auc_roc,\n",
    "    auc_yc,\n",
    "    ece,\n",
    "    nce,\n",
    "    save_confidence_hist,\n",
    "    save_custom_confidence_curve,\n",
    "    save_nt_curve,\n",
    "    save_pr_curve,\n",
    "    save_roc_curve,\n",
    ")\n",
    "\n",
    "\n",
    "targets_with_confidence = [get_word_targets_with_confidence(tran) for tran in transcriptions]\n",
    "correct_marks = [get_correct_marks(r.split(), h.words) for r, h in zip(test_sets.reference_texts, transcriptions)]\n",
    "\n",
    "y_true, y_score = np.array(\n",
    "    [[f, p[1]] for cm, twc in zip(correct_marks, targets_with_confidence) for f, p in zip(cm, twc)]\n",
    ").T\n",
    "\n",
    "\n",
    "# output scheme: yc.mean(), yc.max(), yc.std() or yc.mean(), yc.max(), yc.std(), (thresholds, yc)\n",
    "result_yc = auc_yc(y_true, y_score, return_std_maximum=True, return_curve=True)\n",
    "# output scheme: ece or ece, (thresholds, ece_curve)\n",
    "results_ece = ece(y_true, y_score, return_curve=True)\n",
    "results = [\n",
    "    auc_roc(y_true, y_score),\n",
    "    auc_pr(y_true, y_score),\n",
    "    auc_nt(y_true, y_score),\n",
    "    nce(y_true, y_score),\n",
    "    results_ece[0],\n",
    "] + list(result_yc[:3])\n",
    "\n",
    "print(\n",
    "    f\"\"\"    \n",
    "    AUC_ROC:\\t{results[0]:.5f}\n",
    "    AUC_PR:\\t{results[1]:.5f}\n",
    "    AUC_NT:\\t{results[2]:.5f}\n",
    "    NCE:\\t{results[3]:.5f}\n",
    "    ECE:\\t{results[4]:.5f}\n",
    "    AUC_YC:\\t{results[5]:.5f}\n",
    "    MAX_YC:\\t{results[7]:.5f}\n",
    "    STD_YC:\\t{results[6]:.5f}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3f6299",
   "metadata": {
    "id": "4159034d"
   },
   "source": [
    "Confidence metrics for the maximum probability confidence are not that great.\n",
    "\n",
    "Let's re-run and benchmark confidence estimation with the default confidence estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e3a9f",
   "metadata": {
    "id": "d2e16f5f"
   },
   "outputs": [],
   "source": [
    "confidence_cfg = ConfidenceConfig(\n",
    "    preserve_word_confidence=True,\n",
    "    preserve_token_confidence=True,\n",
    ")\n",
    "\n",
    "model.change_decoding_strategy(\n",
    "    RNNTDecodingConfig(fused_batch_size=-1, strategy=\"greedy_batch\", confidence_cfg=confidence_cfg)\n",
    "    if is_rnnt\n",
    "    else CTCDecodingConfig(confidence_cfg=confidence_cfg)\n",
    ")\n",
    "\n",
    "transcriptions = model.transcribe(paths2audio_files=test_sets.filepaths, batch_size=16, return_hypotheses=True, num_workers=4)\n",
    "if is_rnnt:\n",
    "    transcriptions = transcriptions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f1cc77",
   "metadata": {
    "id": "6201ea4d"
   },
   "outputs": [],
   "source": [
    "targets_with_confidence = [get_word_targets_with_confidence(tran) for tran in transcriptions]\n",
    "correct_marks = [get_correct_marks(r.split(), h.words) for r, h in zip(test_sets.reference_texts, transcriptions)]\n",
    "\n",
    "y_true, y_score = np.array(\n",
    "    [[f, p[1]] for cm, twc in zip(correct_marks, targets_with_confidence) for f, p in zip(cm, twc)]\n",
    ").T\n",
    "\n",
    "result_yc = auc_yc(y_true, y_score, return_std_maximum=True, return_curve=True)\n",
    "results_ece = ece(y_true, y_score, return_curve=True)\n",
    "results = [\n",
    "    auc_roc(y_true, y_score),\n",
    "    auc_pr(y_true, y_score),\n",
    "    auc_nt(y_true, y_score),\n",
    "    nce(y_true, y_score),\n",
    "    results_ece[0],\n",
    "] + list(result_yc[:3])\n",
    "\n",
    "print(\n",
    "    f\"\"\"    AUC_ROC:\\t{results[0]:.5f}\n",
    "    AUC_PR:\\t{results[1]:.5f}\n",
    "    AUC_NT:\\t{results[2]:.5f}\n",
    "    NCE:\\t{results[3]:.5f}\n",
    "    ECE:\\t{results[4]:.5f}\n",
    "    AUC_YC:\\t{results[5]:.5f}\n",
    "    MAX_YC:\\t{results[7]:.5f}\n",
    "    STD_YC:\\t{results[6]:.5f}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f93601",
   "metadata": {},
   "source": [
    "Get the actual error label for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb2fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_fault_scores_jiwer(reference_sentences, hypothesis_sentences):\n",
    "    combined_word_scores = []\n",
    "\n",
    "    for reference_sentence, hypothesis_sentence in zip(reference_sentences, hypothesis_sentences):\n",
    "        # Convert both sentences to lowercase\n",
    "        try:\n",
    "            reference_sentence_lower = reference_sentence.lower()\n",
    "            hypothesis_sentence_lower = hypothesis_sentence.lower()\n",
    "        except:\n",
    "            print(reference_sentence)\n",
    "            print(hypothesis_sentence)\n",
    "\n",
    "        # Process the sentences to get the alignment\n",
    "        alignment_output = jiwer.process_words(reference_sentence_lower, hypothesis_sentence_lower)\n",
    "\n",
    "        # Initialize a list to store the words and scores (including insertions)\n",
    "        word_scores = []\n",
    "\n",
    "        # Current index in the reference words\n",
    "        ref_idx = 0\n",
    "        reference_words = reference_sentence_lower.split()\n",
    "\n",
    "        # Process the alignment to determine the scores\n",
    "        for alignment_chunk in alignment_output.alignments[0]:\n",
    "            if alignment_chunk.type == 'equal':\n",
    "                # Add corrected word with score 0\n",
    "                for idx in range(ref_idx, alignment_chunk.ref_end_idx):\n",
    "                    word_scores.append((reference_words[idx], 0))\n",
    "                ref_idx = alignment_chunk.ref_end_idx\n",
    "\n",
    "            elif alignment_chunk.type == 'substitute':\n",
    "                # Add substituted word with score 1\n",
    "                for idx in range(ref_idx, alignment_chunk.ref_end_idx):\n",
    "                    word_scores.append((reference_words[idx], 1))\n",
    "                ref_idx = alignment_chunk.ref_end_idx\n",
    "\n",
    "            elif alignment_chunk.type == 'delete':\n",
    "                # Add deleted word with score 2\n",
    "                for idx in range(ref_idx, alignment_chunk.ref_end_idx):\n",
    "                    word_scores.append((reference_words[idx], 2))\n",
    "                ref_idx = alignment_chunk.ref_end_idx\n",
    "\n",
    "            elif alignment_chunk.type == 'insert':\n",
    "                # Add \"inserted\" with score 3\n",
    "                for idx in range(ref_idx, alignment_chunk.hyp_end_idx):\n",
    "                    word_scores.append((\"inserted\", 3))\n",
    "                ref_idx = alignment_chunk.ref_end_idx\n",
    "\n",
    "        # Append remaining correct words\n",
    "        for idx in range(ref_idx, len(reference_words)):\n",
    "            word_scores.append((reference_words[idx], 0))\n",
    "\n",
    "        # Append the results for this sentence pair to the combined list\n",
    "        combined_word_scores.append(word_scores)\n",
    "\n",
    "    return combined_word_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16b80436",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_score_word = get_word_fault_scores_jiwer(list(combined_data['referenceText']), list(combined_data['data_transcriptions']))\n",
    "combined_data['jiwer_scores'] = b_score_word\n",
    "\n",
    "combined_data['actualwords'] = combined_data['jiwer_scores'].apply(lambda x: [item[0] for item in x])\n",
    "combined_data['word_jiwer_score'] = combined_data['jiwer_scores'].apply(lambda x: [item[1] for item in x])\n",
    "\n",
    "combined_data.to_csv('/NoRefER/audio_data/CTC.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2486593e",
   "metadata": {},
   "source": [
    "Calculate AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701c08cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "def get_valid_scores_and_attentions(word_jiwer_scores, word_conf):\n",
    "    valid_scores = []\n",
    "    valid_confs = []\n",
    "    for jiwer_score, conf in zip(word_jiwer_scores, word_conf):\n",
    "        if jiwer_score not in [2]:  # Excluding deletion (2) \n",
    "            if conf is not None:\n",
    "                valid_scores.append(1 if jiwer_score != 0 else 0)  # Convert to binary label\n",
    "                valid_confs.append(conf)\n",
    "    return valid_scores, valid_confs\n",
    "\n",
    "auc_scores = []\n",
    "\n",
    "for index, row in combined_data.iterrows():\n",
    "    valid_scores, valid_confs = get_valid_scores_and_attentions(row['word_jiwer_score'], row['confidence_word'])\n",
    "\n",
    "    if len(valid_scores) > 1 and len(valid_confs) > 1:\n",
    "        try:\n",
    "            auc_score = roc_auc_score(valid_scores, valid_confs)\n",
    "            auc_scores.append(auc_score)\n",
    "        except ValueError as e:\n",
    "            # Handle case where only one class is present in y_true\n",
    "            # print(f\"Row {index} skipped: {e}\")\n",
    "            pass\n",
    "\n",
    "average_auc_score = np.nanmean(auc_scores)\n",
    "print(\"Average AUC Score: \", average_auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0d6c14",
   "metadata": {},
   "source": [
    "Calculate AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99af6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "def get_valid_scores_and_attentions(word_jiwer_scores, word_attentions):\n",
    "    valid_scores = []\n",
    "    valid_attentions = []\n",
    "    for jiwer_score, attention in zip(word_jiwer_scores, word_attentions):\n",
    "        if jiwer_score not in [2]:  # Excluding deletion (2) \n",
    "            if attention is not None:\n",
    "                # Ensure that jiwer_score is a valid integer and attention is a valid float\n",
    "                try:\n",
    "                    valid_scores.append(1 if int(jiwer_score) != 0 else 0)  # Convert to binary label\n",
    "                    valid_attentions.append(float(attention))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    return valid_scores, valid_attentions\n",
    "\n",
    "average_precision_scores = []\n",
    "\n",
    "for index, row in combined_data.iterrows():\n",
    "    valid_scores, valid_attentions = get_valid_scores_and_attentions(row['word_jiwer_score'], row['confidence_word'])\n",
    "\n",
    "    if len(valid_scores) > 1 and len(valid_attentions) > 1:\n",
    "        try:\n",
    "            ap_score = average_precision_score(valid_scores, valid_attentions, average='weighted')\n",
    "            average_precision_scores.append(ap_score)\n",
    "        except ValueError as e:\n",
    "            # Handle cases where only one class is present or other issues\n",
    "            # print(f\"Row {index} skipped: {e}\")\n",
    "            pass\n",
    "\n",
    "average_ap_score = np.nanmean(average_precision_scores)\n",
    "print(\"Average AP Score: \", average_ap_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648fb0cb",
   "metadata": {},
   "source": [
    "Calculate top k classification metrics - dynamic k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ed4686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def contains_only_0_and_1(lst):\n",
    "    return all(item in [0, 1] for item in lst)\n",
    "\n",
    "def classify_top_k_conf_words(word_jiwer_scores, word_attentions, sentence_length):\n",
    "    k = max(1, int(np.ceil(0.10 * sentence_length)))  # Ensure at least 1\n",
    "    numeric_conf = [float(att) if att not in [None, 'None'] and isinstance(att, (float, str, int)) else 0 for att in word_attentions]\n",
    "    paired_scores = sorted(zip(word_jiwer_scores, numeric_conf), key=lambda x: x[1])  # Sort in ascending order\n",
    "    \n",
    "    binary_labels = [1 if score[0] != 0 else 0 for score in paired_scores]\n",
    "    binary_predictions = [1 if i < k else 0 for i in range(len(paired_scores))]  # Label lowest k words as faulty\n",
    "\n",
    "    return binary_labels, binary_predictions\n",
    "\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "baccuracy_scores = []\n",
    "\n",
    "# Assume data_attention is a pre-defined DataFrame with the necessary columns\n",
    "# Process each row\n",
    "for index, row in combined_data.iterrows():\n",
    "    word_jiwer_scores = row['word_jiwer_score']\n",
    "    word_attentions = row['confidence_word']\n",
    "    sentence_length = len(word_attentions)  # Assuming 'sentence' column contains the full sentence text\n",
    "\n",
    "    binary_labels, binary_predictions = classify_top_k_conf_words(word_jiwer_scores, word_attentions, sentence_length)\n",
    "\n",
    "    \n",
    "    report = classification_report(binary_labels, binary_predictions, output_dict=True, zero_division=0)\n",
    "    acc = balanced_accuracy_score(binary_labels, binary_predictions)\n",
    "\n",
    "    precision_scores.append(report['weighted avg']['precision'])\n",
    "    recall_scores.append(report['weighted avg']['recall'])\n",
    "    f1_scores.append(report['weighted avg']['f1-score'])\n",
    "    accuracy_scores.append(report['accuracy'])\n",
    "    baccuracy_scores.append(acc)\n",
    "    \n",
    "          \n",
    "\n",
    "# Calculate mean of metrics\n",
    "mean_precision = np.nanmean(precision_scores)\n",
    "mean_recall = np.nanmean(recall_scores)\n",
    "mean_f1 = np.nanmean(f1_scores)\n",
    "mean_accuracy = np.nanmean(accuracy_scores)\n",
    "mean_baccuracy = np.nanmean(baccuracy_scores)\n",
    "\n",
    "print(f\"Mean Precision: {mean_precision}\")\n",
    "print(f\"Mean Recall: {mean_recall}\")\n",
    "print(f\"Mean F1 Score: {mean_f1}\")\n",
    "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
    "print(f\"Mean Balanced Accuracy: {mean_baccuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
