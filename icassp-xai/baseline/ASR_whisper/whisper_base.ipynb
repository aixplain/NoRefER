{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "Assuming to have the audio data dowloded in following directory:\n",
    "\n",
    "/NoRefER/audio_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from whisper_baseline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = '/NoRefER/'\n",
    "DATA_DIR = WORK_DIR + '/audio_data/'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of filenames\n",
    "filenames = ['en-libre.csv', 'en-common.csv', 'es-common.csv', 'fr-common.csv']\n",
    "folder_path = '../../data/'\n",
    "\n",
    "# List to store data from each file\n",
    "all_data = []\n",
    "\n",
    "# Loop through each file and load data\n",
    "for filename in filenames:\n",
    "    print(f'Start processing file: {filename}')\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(f\"Data loaded successfully for {filename}.\")\n",
    "        all_data.append(data)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading {filename}:\", e)\n",
    "        continue\n",
    "\n",
    "# Concatenate all data into a single DataFrame\n",
    "combined_data = pd.concat(all_data, ignore_index=True)\n",
    "audio_path_s3 = combined_data['inputText'].astype(str).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TestSet:\n",
    "    filepaths: List[str]\n",
    "    reference_texts: List[str]\n",
    "\n",
    "def load_data(df):\n",
    "    filepaths = []\n",
    "    reference_texts = []\n",
    "    durations = []\n",
    "    for index, row in df.iterrows():\n",
    "        audio_file = row['local_paths']\n",
    "        filepaths.append(str(audio_file))\n",
    "        text = row['referenceText']\n",
    "        reference_texts.append(text)\n",
    "    return TestSet(filepaths, reference_texts)\n",
    "\n",
    "# Load data\n",
    "test_sets =  load_data(combined_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up confidence estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store transcription results\n",
    "transcription_dict = {}\n",
    "\n",
    "# Get unique filepaths\n",
    "unique_filepaths = set(test_sets.filepaths)\n",
    "\n",
    "# Transcribe each unique audio file and store the results\n",
    "for audio in unique_filepaths:\n",
    "    prob, transcription = whisper_probs(audio)\n",
    "    transcription_dict[audio] = {'prob': prob, 'transcription': transcription}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/NoRefERn/baseline/whisper/transcription_dict', 'w') as json_file:\n",
    "    json.dump(transcription_dict, json_file, indent=4)\n",
    "\n",
    "\n",
    "# import json\n",
    "# with open('/NoRefERn/baseline/whisper/transcription_dict', 'r') as json_file:\n",
    "#     transcription_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the results back to the original DataFrame\n",
    "combined_data['probs'] = combined_data['local_paths'].map(lambda x: transcription_dict[x]['prob'])\n",
    "combined_data['data_transcriptions'] = combined_data['local_paths'].map(lambda x: transcription_dict[x]['transcription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = process_transcription_attention(combined_data, combined_data['data_transcriptions'] , need_split=False) \n",
    "combined_data = calculate_word_scores_with_tokens(combined_data, 'probs', aggregation_method='max')  # aggregation_method=['average', 'max', 'q3']\n",
    "\n",
    "b_score_word = get_word_fault_scores_jiwer(list(combined_data['referenceText']), list(combined_data['data_transcriptions']))\n",
    "combined_data['jiwer_scores'] = b_score_word\n",
    "\n",
    "combined_data['actualwords'] = combined_data['jiwer_scores'].apply(lambda x: [item[0] for item in x])\n",
    "combined_data['word_jiwer_score'] = combined_data['jiwer_scores'].apply(lambda x: [item[1] for item in x])\n",
    "combined_data['prob_aligned'] = align_attention_with_jiwer(combined_data['word_jiwer_score'], combined_data['word_attentions'])\n",
    "\n",
    "combined_data.to_csv('/NoRefER/audio_data/whisper.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "def get_valid_scores_and_attentions(word_jiwer_scores, word_attentions):\n",
    "    valid_scores = []\n",
    "    valid_attentions = []\n",
    "    for jiwer_score, attention in zip(word_jiwer_scores, word_attentions):\n",
    "        if jiwer_score not in [2]:  # Excluding deletion (2) and insertion (3)\n",
    "            if attention is not None:\n",
    "                valid_scores.append(1 if jiwer_score != 0 else 0)  # Convert to binary label\n",
    "                valid_attentions.append(attention)\n",
    "    return valid_scores, valid_attentions\n",
    "\n",
    "auc_scores = []\n",
    "\n",
    "for index, row in combined_data.iterrows():\n",
    "    valid_scores, valid_attentions = get_valid_scores_and_attentions(row['word_jiwer_score'], row['prob_aligned'])\n",
    "\n",
    "    if len(valid_scores) > 1 and len(valid_attentions) > 1:\n",
    "        try:\n",
    "            auc_score = roc_auc_score(valid_scores, valid_attentions)\n",
    "            auc_scores.append(auc_score)\n",
    "        except ValueError as e:\n",
    "            # Handle case where only one class is present in y_true\n",
    "            # print(f\"Row {index} skipped: {e}\")\n",
    "            pass\n",
    "\n",
    "average_auc_score = np.nanmean(auc_scores)\n",
    "print(\"Average AUC Score: \", average_auc_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "def get_valid_scores_and_attentions(word_jiwer_scores, word_attentions):\n",
    "    valid_scores = []\n",
    "    valid_attentions = []\n",
    "    for jiwer_score, attention in zip(word_jiwer_scores, word_attentions):\n",
    "        if jiwer_score not in [2]:  # Excluding deletion (2) \n",
    "            if attention is not None:\n",
    "                # Ensure that jiwer_score is a valid integer and attention is a valid float\n",
    "                try:\n",
    "                    valid_scores.append(1 if int(jiwer_score) != 0 else 0)  # Convert to binary label\n",
    "                    valid_attentions.append(float(attention))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    return valid_scores, valid_attentions\n",
    "\n",
    "average_precision_scores = []\n",
    "\n",
    "for index, row in combined_data.iterrows():\n",
    "    valid_scores, valid_attentions = get_valid_scores_and_attentions(row['word_jiwer_score'], row['prob_aligned'])\n",
    "\n",
    "    if len(valid_scores) > 1 and len(valid_attentions) > 1:\n",
    "        try:\n",
    "            ap_score = average_precision_score(valid_scores, valid_attentions, average='weighted')\n",
    "            average_precision_scores.append(ap_score)\n",
    "        except ValueError as e:\n",
    "            # Handle cases where only one class is present or other issues\n",
    "            # print(f\"Row {index} skipped: {e}\")\n",
    "            pass\n",
    "\n",
    "average_ap_score = np.nanmean(average_precision_scores)\n",
    "print(\"Average AP Score: \", average_ap_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate top k classification metrics - dynamic k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def classify_top_k_attention_words(word_jiwer_scores, word_attentions, sentence_length):\n",
    "    # Dynamic k based on 10% of sentence length\n",
    "    k = max(1, int(np.ceil(0.10 * sentence_length)))  # Ensure at least 1\n",
    "    numeric_attentions = [float(att) if att not in [None, 'None'] and isinstance(att, (float, str, int)) else 0 for att in word_attentions]\n",
    "    paired_scores = sorted(zip(word_jiwer_scores, numeric_attentions), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    binary_labels = [1 if score[0] != 0 else 0 for score in paired_scores]  # Convert word_jiwer_scores to binary\n",
    "    binary_predictions = [1 if i < k else 0 for i in range(len(paired_scores))]  # Top k words are faulty\n",
    "\n",
    "    return binary_labels, binary_predictions\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "baccuracy_scores = []\n",
    "\n",
    "# Assume data_attention is a pre-defined DataFrame with the necessary columns\n",
    "# Process each row\n",
    "for index, row in combined_data.iterrows():\n",
    "    word_jiwer_scores = row['word_jiwer_score']\n",
    "    word_attentions = row['prob_aligned']\n",
    "    sentence_length = len(word_attentions)  # Assuming 'sentence' column contains the full sentence text\n",
    "\n",
    "    binary_labels, binary_predictions = classify_top_k_attention_words(word_jiwer_scores, word_attentions, sentence_length)\n",
    "\n",
    "    report = classification_report(binary_labels, binary_predictions, output_dict=True, zero_division=0)\n",
    "    acc = balanced_accuracy_score(binary_labels, binary_predictions)\n",
    "\n",
    "    precision_scores.append(report['weighted avg']['precision'])\n",
    "    recall_scores.append(report['weighted avg']['recall'])\n",
    "    f1_scores.append(report['weighted avg']['f1-score'])\n",
    "    accuracy_scores.append(report['accuracy'])\n",
    "    baccuracy_scores.append(acc)\n",
    "\n",
    "# Calculate mean of metrics\n",
    "mean_precision = np.mean(precision_scores)\n",
    "mean_recall = np.mean(recall_scores)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "mean_baccuracy = np.mean(baccuracy_scores)\n",
    "\n",
    "print(f\"Mean Precision: {mean_precision}\")\n",
    "print(f\"Mean Recall: {mean_recall}\")\n",
    "print(f\"Mean F1 Score: {mean_f1}\")\n",
    "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
    "print(f\"Mean Balanced Accuracy: {mean_baccuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asrvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
